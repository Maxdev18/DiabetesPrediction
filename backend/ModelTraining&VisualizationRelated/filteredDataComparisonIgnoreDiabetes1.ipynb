{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33109e7f802b7b32",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T00:13:54.117398100Z",
     "start_time": "2023-10-27T00:13:52.306897700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'filtered_normalized_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assign the binary diabetes label\n",
    "negative = df[df['Diabetes_012'] == 0]\n",
    "positive = df[df['Diabetes_012'] == 2]\n",
    "\n",
    "# Get the smallest class size\n",
    "min_class_size = min(len(negative), len(positive))\n",
    "\n",
    "# Resample each class to have the same size as the smallest class\n",
    "negative_downsampled = resample(negative, replace=False, n_samples=min_class_size, random_state=42)\n",
    "positive_downsampled = resample(positive, replace=False, n_samples=min_class_size, random_state=42)\n",
    "\n",
    "# Concatenate all downsampled classes to create a balanced dataset\n",
    "downsampled_data = pd.concat([negative_downsampled, positive_downsampled])\n",
    "\n",
    "# The downsized dataset conains 70692 rows\n",
    "\n",
    "X = downsampled_data.drop('Diabetes_012', axis=1).values\n",
    "y = downsampled_data['Diabetes_012'].values\n",
    "\n",
    "#k-fold k=5\n",
    "kf = KFold(n_splits=5, random_state=445, shuffle=True)\n",
    "n_splits=5\n",
    "\n",
    "# Create X_full and y_full from the full dataset (Added)\n",
    "X_full = df.drop('Diabetes_012', axis=1).values\n",
    "y_full = df['Diabetes_012'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b34e0947d42a2aa",
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-10-27T00:14:46.770904800Z",
     "start_time": "2023-10-27T00:13:54.122409500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.69      0.71      7001\n",
      "           2       0.71      0.75      0.73      7138\n",
      "\n",
      "    accuracy                           0.72     14139\n",
      "   macro avg       0.72      0.72      0.72     14139\n",
      "weighted avg       0.72      0.72      0.72     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.72      7067\n",
      "           2       0.71      0.76      0.74      7072\n",
      "\n",
      "    accuracy                           0.73     14139\n",
      "   macro avg       0.73      0.73      0.73     14139\n",
      "weighted avg       0.73      0.73      0.73     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71      7125\n",
      "           2       0.70      0.76      0.73      7013\n",
      "\n",
      "    accuracy                           0.72     14138\n",
      "   macro avg       0.72      0.72      0.72     14138\n",
      "weighted avg       0.72      0.72      0.72     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.71      7026\n",
      "           2       0.71      0.76      0.74      7112\n",
      "\n",
      "    accuracy                           0.73     14138\n",
      "   macro avg       0.73      0.73      0.73     14138\n",
      "weighted avg       0.73      0.73      0.73     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.71      7127\n",
      "           2       0.71      0.74      0.72      7011\n",
      "\n",
      "    accuracy                           0.72     14138\n",
      "   macro avg       0.72      0.72      0.72     14138\n",
      "weighted avg       0.72      0.72      0.72     14138\n",
      "Average Precision: {0: 0.7380146880321999, 2: 0.7091331435121366}\n",
      "Average Recall: {0: 0.69040299648835, 2: 0.7548295013115018}\n",
      "Average F1-Score: {0: 0.7133903383887146, 2: 0.7312477009845286}\n",
      "Average Precision on Full Dataset: {0: 0.9797028495547643, 2: 0.35463297803387767}\n",
      "Average Recall on Full Dataset: {0: 0.726388492440443, 2: 0.909008091438918}\n",
      "Average F1-Score on Full Dataset: {0: 0.8342384499541733, 2: 0.5102123979794353}\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "avg_precision = {0: 0, 2: 0}\n",
    "avg_recall = {0: 0, 2: 0}\n",
    "avg_f1_score = {0: 0, 2: 0}\n",
    "# Initialize accumulators for the full dataset\n",
    "avg_precision_full = {0: 0, 2: 0}\n",
    "avg_recall_full = {0: 0, 2: 0}\n",
    "avg_f1_score_full = {0: 0, 2: 0}\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "    class_to_index = {label: index for index, label in enumerate(sorted(np.unique(y_test)))}\n",
    "\n",
    "    # Accumulate metrics for averaging\n",
    "    for i in [0, 2]:\n",
    "        idx = class_to_index[i]\n",
    "        avg_precision[i] += precision[idx]\n",
    "        avg_recall[i] += recall[idx]\n",
    "        avg_f1_score[i] += f1_score[idx]\n",
    "    #FULL added\n",
    "    y_pred_full = clf.predict(X_full)\n",
    "    # Calculate metrics for the full dataset\n",
    "    precision_full, recall_full, f1_score_full, _ = precision_recall_fscore_support(y_full, y_pred_full, average=None)\n",
    "    class_to_index_full = {label: index for index, label in enumerate(sorted(np.unique(y_full)))}\n",
    "\n",
    "    # Accumulate metrics for averaging (full dataset)\n",
    "    for i in [0, 2]:\n",
    "        idx_full = class_to_index_full[i]\n",
    "        avg_precision_full[i] += precision_full[idx_full]\n",
    "        avg_recall_full[i] += recall_full[idx_full]\n",
    "        avg_f1_score_full[i] += f1_score_full[idx_full]\n",
    "\n",
    "# Calculate average metrics\n",
    "for i in [0, 2]:\n",
    "    avg_precision[i] /= n_splits\n",
    "    avg_recall[i] /= n_splits\n",
    "    avg_f1_score[i] /= n_splits\n",
    "# Calculate average metrics for the full dataset\n",
    "for i in [0, 2]:\n",
    "    avg_precision_full[i] /= n_splits\n",
    "    avg_recall_full[i] /= n_splits\n",
    "    avg_f1_score_full[i] /= n_splits\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1-Score:\", avg_f1_score)\n",
    "\n",
    "# Print average metrics for the full dataset\n",
    "print(\"Average Precision on Full Dataset:\", avg_precision_full)\n",
    "print(\"Average Recall on Full Dataset:\", avg_recall_full)\n",
    "print(\"Average F1-Score on Full Dataset:\", avg_f1_score_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e24ed8d845cf5e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T00:14:47.468397Z",
     "start_time": "2023-10-27T00:14:46.775898600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74      7001\n",
      "           2       0.74      0.77      0.75      7138\n",
      "\n",
      "    accuracy                           0.75     14139\n",
      "   macro avg       0.75      0.75      0.75     14139\n",
      "weighted avg       0.75      0.75      0.75     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74      7067\n",
      "           2       0.74      0.77      0.75      7072\n",
      "\n",
      "    accuracy                           0.75     14139\n",
      "   macro avg       0.75      0.75      0.75     14139\n",
      "weighted avg       0.75      0.75      0.75     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75      7125\n",
      "           2       0.74      0.76      0.75      7013\n",
      "\n",
      "    accuracy                           0.75     14138\n",
      "   macro avg       0.75      0.75      0.75     14138\n",
      "weighted avg       0.75      0.75      0.75     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74      7026\n",
      "           2       0.74      0.77      0.76      7112\n",
      "\n",
      "    accuracy                           0.75     14138\n",
      "   macro avg       0.75      0.75      0.75     14138\n",
      "weighted avg       0.75      0.75      0.75     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74      7127\n",
      "           2       0.74      0.75      0.74      7011\n",
      "\n",
      "    accuracy                           0.74     14138\n",
      "   macro avg       0.74      0.74      0.74     14138\n",
      "weighted avg       0.74      0.74      0.74     14138\n",
      "\n",
      "Average Precision: {0: 0.7561630080365627, 2: 0.7397563100471972}\n",
      "Average Recall: {0: 0.7311923167702652, 2: 0.7641569838660954}\n",
      "Average F1-Score: {0: 0.7434619743504841, 2: 0.7517535664331285}\n",
      "Average Precision on Full Dataset: {0: 0.9493345840076227, 2: 0.3183739894375079}\n",
      "Average Recall on Full Dataset: {0: 0.7292101655100771, 2: 0.7647032196005206}\n",
      "Average F1-Score on Full Dataset: {0: 0.824838638562858, 2: 0.4495735871665688}\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression\n",
    "clf = LogisticRegression(max_iter=3000, solver='lbfgs')\n",
    "\n",
    "avg_precision = {0: 0, 2: 0}\n",
    "avg_recall = {0: 0, 2: 0}\n",
    "avg_f1_score = {0: 0, 2: 0}\n",
    "# Initialize accumulators for the full dataset\n",
    "avg_precision_full = {0: 0, 2: 0}\n",
    "avg_recall_full = {0: 0, 2: 0}\n",
    "avg_f1_score_full = {0: 0, 2: 0}\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=1)\n",
    "    class_to_index = {label: index for index, label in enumerate(sorted(np.unique(y_test)))}\n",
    "\n",
    "    # Accumulate metrics for averaging\n",
    "    for i in [0, 2]:\n",
    "        idx = class_to_index[i]\n",
    "        avg_precision[i] += precision[idx]\n",
    "        avg_recall[i] += recall[idx]\n",
    "        avg_f1_score[i] += f1_score[idx]\n",
    "    #FULL added\n",
    "    y_pred_full = clf.predict(X_full)\n",
    "    # Calculate metrics for the full dataset\n",
    "    precision_full, recall_full, f1_score_full, _ = precision_recall_fscore_support(y_full, y_pred_full, average=None)\n",
    "    class_to_index_full = {label: index for index, label in enumerate(sorted(np.unique(y_full)))}\n",
    "\n",
    "    # Accumulate metrics for averaging (full dataset)\n",
    "    for i in [0, 2]:\n",
    "        idx_full = class_to_index_full[i]\n",
    "        avg_precision_full[i] += precision_full[idx_full]\n",
    "        avg_recall_full[i] += recall_full[idx_full]\n",
    "        avg_f1_score_full[i] += f1_score_full[idx_full]\n",
    "\n",
    "# Calculate average metrics\n",
    "for i in [0, 2]:\n",
    "    avg_precision[i] /= n_splits\n",
    "    avg_recall[i] /= n_splits\n",
    "    avg_f1_score[i] /= n_splits\n",
    "# Calculate average metrics for the full dataset\n",
    "for i in [0, 2]:\n",
    "    avg_precision_full[i] /= n_splits\n",
    "    avg_recall_full[i] /= n_splits\n",
    "    avg_f1_score_full[i] /= n_splits\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1-Score:\", avg_f1_score)\n",
    "\n",
    "# Print average metrics for the full dataset\n",
    "print(\"Average Precision on Full Dataset:\", avg_precision_full)\n",
    "print(\"Average Recall on Full Dataset:\", avg_recall_full)\n",
    "print(\"Average F1-Score on Full Dataset:\", avg_f1_score_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac0ee191cfe3661",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T00:14:47.490397300Z",
     "start_time": "2023-10-27T00:14:47.469399200Z"
    }
   },
   "outputs": [],
   "source": [
    "# #SVM\n",
    "# clf = SVC(C=1.0, kernel='rbf')\n",
    "# \n",
    "# avg_precision = {0: 0, 1: 0, 2: 0}\n",
    "# avg_recall = {0: 0, 1: 0, 2: 0}\n",
    "# avg_f1_score = {0: 0, 1: 0, 2: 0}\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "# \n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "# \n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     # Calculate metrics\n",
    "#     precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "#     class_to_index = {label: index for index, label in enumerate(sorted(np.unique(y_test)))}\n",
    "# \n",
    "#     # Accumulate metrics for averaging\n",
    "#     for i in [0, 2]:\n",
    "#         avg_precision[i] += precision[i]\n",
    "#         avg_recall[i] += recall[i]\n",
    "#         avg_f1_score[i] += f1_score[i]\n",
    "# \n",
    "# # Calculate average metrics\n",
    "# for i in [0, 2]:\n",
    "#     idx = class_to_index[i]\n",
    "#     avg_precision[i] /= n_splits\n",
    "#     avg_recall[i] /= n_splits\n",
    "#     avg_f1_score[i] /= n_splits\n",
    "# \n",
    "# # Print average metrics\n",
    "# print(\"Average Precision:\", avg_precision)\n",
    "# print(\"Average Recall:\", avg_recall)\n",
    "# print(\"Average F1-Score:\", avg_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f41ac5e50993fcc8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T00:20:05.916897700Z",
     "start_time": "2023-10-27T00:14:47.489397500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71      7001\n",
      "           2       0.72      0.73      0.72      7138\n",
      "\n",
      "    accuracy                           0.72     14139\n",
      "   macro avg       0.72      0.72      0.72     14139\n",
      "weighted avg       0.72      0.72      0.72     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71      7067\n",
      "           2       0.71      0.73      0.72      7072\n",
      "\n",
      "    accuracy                           0.71     14139\n",
      "   macro avg       0.71      0.71      0.71     14139\n",
      "weighted avg       0.71      0.71      0.71     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.71      7125\n",
      "           2       0.70      0.74      0.72      7013\n",
      "\n",
      "    accuracy                           0.72     14138\n",
      "   macro avg       0.72      0.72      0.72     14138\n",
      "weighted avg       0.72      0.72      0.72     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71      7026\n",
      "           2       0.71      0.73      0.72      7112\n",
      "\n",
      "    accuracy                           0.71     14138\n",
      "   macro avg       0.71      0.71      0.71     14138\n",
      "weighted avg       0.71      0.71      0.71     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71      7127\n",
      "           2       0.70      0.71      0.71      7011\n",
      "\n",
      "    accuracy                           0.71     14138\n",
      "   macro avg       0.71      0.71      0.71     14138\n",
      "weighted avg       0.71      0.71      0.71     14138\n",
      "Average Precision: {0: 0.7194741867105334, 2: 0.7079946186635995}\n",
      "Average Recall: {0: 0.7002430942326834, 2: 0.7269108636369774}\n",
      "Average F1-Score: {0: 0.7097123612381363, 2: 0.7173128697964147}\n",
      "Average Precision on Full Dataset: {0: 0.9539658647502822, 2: 0.3143308481383301}\n",
      "Average Recall on Full Dataset: {0: 0.7143952120466255, 2: 0.7915690601482488}\n",
      "Average F1-Score on Full Dataset: {0: 0.8169771057794412, 2: 0.4499726964012315}\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "clf = KNeighborsClassifier(n_neighbors=5, algorithm='auto')\n",
    "\n",
    "avg_precision = {0: 0, 2: 0}\n",
    "avg_recall = {0: 0, 2: 0}\n",
    "avg_f1_score = {0: 0, 2: 0}\n",
    "# Initialize accumulators for the full dataset\n",
    "avg_precision_full = {0: 0, 2: 0}\n",
    "avg_recall_full = {0: 0, 2: 0}\n",
    "avg_f1_score_full = {0: 0, 2: 0}\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "    class_to_index = {label: index for index, label in enumerate(sorted(np.unique(y_test)))}\n",
    "\n",
    "    # Accumulate metrics for averaging\n",
    "    for i in [0, 2]:\n",
    "        idx = class_to_index[i]\n",
    "        avg_precision[i] += precision[idx]\n",
    "        avg_recall[i] += recall[idx]\n",
    "        avg_f1_score[i] += f1_score[idx]\n",
    "    #FULL added\n",
    "    y_pred_full = clf.predict(X_full)\n",
    "    # Calculate metrics for the full dataset\n",
    "    precision_full, recall_full, f1_score_full, _ = precision_recall_fscore_support(y_full, y_pred_full, average=None)\n",
    "    class_to_index_full = {label: index for index, label in enumerate(sorted(np.unique(y_full)))}\n",
    "\n",
    "    # Accumulate metrics for averaging (full dataset)\n",
    "    for i in [0, 2]:\n",
    "        idx_full = class_to_index_full[i]\n",
    "        avg_precision_full[i] += precision_full[idx_full]\n",
    "        avg_recall_full[i] += recall_full[idx_full]\n",
    "        avg_f1_score_full[i] += f1_score_full[idx_full]\n",
    "\n",
    "# Calculate average metrics\n",
    "for i in [0, 2]:\n",
    "    avg_precision[i] /= n_splits\n",
    "    avg_recall[i] /= n_splits\n",
    "    avg_f1_score[i] /= n_splits\n",
    "# Calculate average metrics for the full dataset\n",
    "for i in [0, 2]:\n",
    "    avg_precision_full[i] /= n_splits\n",
    "    avg_recall_full[i] /= n_splits\n",
    "    avg_f1_score_full[i] /= n_splits\n",
    "# Print average metrics\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1-Score:\", avg_f1_score)\n",
    "\n",
    "# Print average metrics for the full dataset\n",
    "print(\"Average Precision on Full Dataset:\", avg_precision_full)\n",
    "print(\"Average Recall on Full Dataset:\", avg_recall_full)\n",
    "print(\"Average F1-Score on Full Dataset:\", avg_f1_score_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "836787a2e4b46727",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T00:20:07.716400400Z",
     "start_time": "2023-10-27T00:20:05.921899200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67      7001\n",
      "           2       0.68      0.64      0.66      7138\n",
      "\n",
      "    accuracy                           0.67     14139\n",
      "   macro avg       0.67      0.67      0.67     14139\n",
      "weighted avg       0.67      0.67      0.67     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68      7067\n",
      "           2       0.68      0.65      0.66      7072\n",
      "\n",
      "    accuracy                           0.67     14139\n",
      "   macro avg       0.67      0.67      0.67     14139\n",
      "weighted avg       0.67      0.67      0.67     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67      7125\n",
      "           2       0.67      0.64      0.65      7013\n",
      "\n",
      "    accuracy                           0.66     14138\n",
      "   macro avg       0.66      0.66      0.66     14138\n",
      "weighted avg       0.66      0.66      0.66     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67      7026\n",
      "           2       0.68      0.64      0.66      7112\n",
      "\n",
      "    accuracy                           0.67     14138\n",
      "   macro avg       0.67      0.67      0.67     14138\n",
      "weighted avg       0.67      0.67      0.67     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68      7127\n",
      "           2       0.67      0.63      0.65      7011\n",
      "\n",
      "    accuracy                           0.66     14138\n",
      "   macro avg       0.66      0.66      0.66     14138\n",
      "weighted avg       0.66      0.66      0.66     14138\n",
      "\n",
      "Average Precision: {0: 0.6575244482974414, 2: 0.6741989792551715}\n",
      "Average Recall: {0: 0.6906072853465245, 2: 0.6402791530276847}\n",
      "Average F1-Score: {0: 0.6736536856879062, 2: 0.65679470190253}\n",
      "Average Precision on Full Dataset: {0: 0.9711702089099805, 2: 0.34766593661732326}\n",
      "Average Recall on Full Dataset: {0: 0.7303360271030355, 2: 0.8689186895263962}\n",
      "Average F1-Score on Full Dataset: {0: 0.833708377117756, 2: 0.4966242898137548}\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "clf = DecisionTreeClassifier(criterion='gini', splitter='best', random_state=42)\n",
    "\n",
    "avg_precision = {0: 0, 2: 0}\n",
    "avg_recall = {0: 0, 2: 0}\n",
    "avg_f1_score = {0: 0,2: 0}\n",
    "# Initialize accumulators for the full dataset\n",
    "avg_precision_full = {0: 0, 2: 0}\n",
    "avg_recall_full = {0: 0, 2: 0}\n",
    "avg_f1_score_full = {0: 0, 2: 0}\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "    class_to_index = {label: index for index, label in enumerate(sorted(np.unique(y_test)))}\n",
    "\n",
    "    # Accumulate metrics for averaging\n",
    "    for i in [0, 2]:\n",
    "        idx = class_to_index[i]\n",
    "        avg_precision[i] += precision[idx]\n",
    "        avg_recall[i] += recall[idx]\n",
    "        avg_f1_score[i] += f1_score[idx]\n",
    "    #FULL added\n",
    "    y_pred_full = clf.predict(X_full)\n",
    "    # Calculate metrics for the full dataset\n",
    "    precision_full, recall_full, f1_score_full, _ = precision_recall_fscore_support(y_full, y_pred_full, average=None)\n",
    "    class_to_index_full = {label: index for index, label in enumerate(sorted(np.unique(y_full)))}\n",
    "\n",
    "    # Accumulate metrics for averaging (full dataset)\n",
    "    for i in [0, 2]:\n",
    "        idx_full = class_to_index_full[i]\n",
    "        avg_precision_full[i] += precision_full[idx_full]\n",
    "        avg_recall_full[i] += recall_full[idx_full]\n",
    "        avg_f1_score_full[i] += f1_score_full[idx_full]\n",
    "\n",
    "# Calculate average metrics\n",
    "for i in [0, 2]:\n",
    "    avg_precision[i] /= n_splits\n",
    "    avg_recall[i] /= n_splits\n",
    "    avg_f1_score[i] /= n_splits\n",
    "# Calculate average metrics for the full dataset\n",
    "for i in [0, 2]:\n",
    "    avg_precision_full[i] /= n_splits\n",
    "    avg_recall_full[i] /= n_splits\n",
    "    avg_f1_score_full[i] /= n_splits\n",
    "# Print average metrics\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1-Score:\", avg_f1_score)\n",
    "\n",
    "# Print average metrics for the full dataset\n",
    "print(\"Average Precision on Full Dataset:\", avg_precision_full)\n",
    "print(\"Average Recall on Full Dataset:\", avg_recall_full)\n",
    "print(\"Average F1-Score on Full Dataset:\", avg_f1_score_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c9485b77dcf15d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T00:20:27.166397800Z",
     "start_time": "2023-10-27T00:20:07.721898600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.74      7001\n",
      "           2       0.74      0.79      0.76      7138\n",
      "\n",
      "    accuracy                           0.75     14139\n",
      "   macro avg       0.75      0.75      0.75     14139\n",
      "weighted avg       0.75      0.75      0.75     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      7067\n",
      "           2       0.73      0.80      0.76      7072\n",
      "\n",
      "    accuracy                           0.75     14139\n",
      "   macro avg       0.75      0.75      0.75     14139\n",
      "weighted avg       0.75      0.75      0.75     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      7125\n",
      "           2       0.73      0.79      0.76      7013\n",
      "\n",
      "    accuracy                           0.75     14138\n",
      "   macro avg       0.75      0.75      0.75     14138\n",
      "weighted avg       0.75      0.75      0.75     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.75      7026\n",
      "           2       0.74      0.80      0.77      7112\n",
      "\n",
      "    accuracy                           0.76     14138\n",
      "   macro avg       0.76      0.76      0.76     14138\n",
      "weighted avg       0.76      0.76      0.76     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74      7127\n",
      "           2       0.73      0.78      0.75      7011\n",
      "\n",
      "    accuracy                           0.75     14138\n",
      "   macro avg       0.75      0.75      0.75     14138\n",
      "weighted avg       0.75      0.75      0.75     14138\n",
      "Average Precision: {0: 0.7736839996811625, 2: 0.7340027303357741}\n",
      "Average Recall: {0: 0.7132700427581892, 2: 0.7912966127577784}\n",
      "Average F1-Score: {0: 0.7422378601935764, 2: 0.761564307452241}\n",
      "Average Precision on Full Dataset: {0: 0.9543132844833082, 2: 0.3137748514920859}\n",
      "Average Recall on Full Dataset: {0: 0.7129127808219818, 2: 0.7936456741922707}\n",
      "Average F1-Score on Full Dataset: {0: 0.8161350027254191, 2: 0.44973833201324825}\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boost\n",
    "clf = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100)\n",
    "\n",
    "avg_precision = {0: 0, 2: 0}\n",
    "avg_recall = {0: 0, 2: 0}\n",
    "avg_f1_score = {0: 0, 2: 0}\n",
    "# Initialize accumulators for the full dataset\n",
    "avg_precision_full = {0: 0, 2: 0}\n",
    "avg_recall_full = {0: 0, 2: 0}\n",
    "avg_f1_score_full = {0: 0, 2: 0}\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "    class_to_index = {label: index for index, label in enumerate(sorted(np.unique(y_test)))}\n",
    "\n",
    "    # Accumulate metrics for averaging\n",
    "    for i in [0, 2]:\n",
    "        idx = class_to_index[i]\n",
    "        avg_precision[i] += precision[idx]\n",
    "        avg_recall[i] += recall[idx]\n",
    "        avg_f1_score[i] += f1_score[idx]\n",
    "    #FULL added\n",
    "    y_pred_full = clf.predict(X_full)\n",
    "    # Calculate metrics for the full dataset\n",
    "    precision_full, recall_full, f1_score_full, _ = precision_recall_fscore_support(y_full, y_pred_full, average=None)\n",
    "    class_to_index_full = {label: index for index, label in enumerate(sorted(np.unique(y_full)))}\n",
    "\n",
    "    # Accumulate metrics for averaging (full dataset)\n",
    "    for i in [0, 2]:\n",
    "        idx_full = class_to_index_full[i]\n",
    "        avg_precision_full[i] += precision_full[idx_full]\n",
    "        avg_recall_full[i] += recall_full[idx_full]\n",
    "        avg_f1_score_full[i] += f1_score_full[idx_full]\n",
    "\n",
    "# Calculate average metrics\n",
    "for i in [0, 2]:\n",
    "    avg_precision[i] /= n_splits\n",
    "    avg_recall[i] /= n_splits\n",
    "    avg_f1_score[i] /= n_splits\n",
    "# Calculate average metrics for the full dataset\n",
    "for i in [0, 2]:\n",
    "    avg_precision_full[i] /= n_splits\n",
    "    avg_recall_full[i] /= n_splits\n",
    "    avg_f1_score_full[i] /= n_splits\n",
    "# Print average metrics\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1-Score:\", avg_f1_score)\n",
    "\n",
    "# Print average metrics for the full dataset\n",
    "print(\"Average Precision on Full Dataset:\", avg_precision_full)\n",
    "print(\"Average Recall on Full Dataset:\", avg_recall_full)\n",
    "print(\"Average F1-Score on Full Dataset:\", avg_f1_score_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21fe7ecb75d1eaab",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T00:20:30.143898200Z",
     "start_time": "2023-10-27T00:20:27.166397800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74      7001\n",
      "           1       0.74      0.79      0.76      7138\n",
      "\n",
      "    accuracy                           0.75     14139\n",
      "   macro avg       0.75      0.75      0.75     14139\n",
      "weighted avg       0.75      0.75      0.75     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      7067\n",
      "           1       0.73      0.80      0.76      7072\n",
      "\n",
      "    accuracy                           0.75     14139\n",
      "   macro avg       0.75      0.75      0.75     14139\n",
      "weighted avg       0.75      0.75      0.75     14139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      7125\n",
      "           1       0.73      0.80      0.76      7013\n",
      "\n",
      "    accuracy                           0.75     14138\n",
      "   macro avg       0.75      0.75      0.75     14138\n",
      "weighted avg       0.75      0.75      0.75     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.74      7026\n",
      "           1       0.73      0.80      0.77      7112\n",
      "\n",
      "    accuracy                           0.75     14138\n",
      "   macro avg       0.76      0.75      0.75     14138\n",
      "weighted avg       0.76      0.75      0.75     14138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.74      7127\n",
      "           1       0.73      0.78      0.75      7011\n",
      "\n",
      "    accuracy                           0.75     14138\n",
      "   macro avg       0.75      0.75      0.75     14138\n",
      "weighted avg       0.75      0.75      0.75     14138\n",
      "\n",
      "Average Precision: {0: 0.774107175670346, 1: 0.7324025265673177}\n",
      "Average Recall: {0: 0.7104017354800212, 1: 0.7926313901238007}\n",
      "Average F1-Score: {0: 0.7408675211202017, 1: 0.7613124494568149}\n",
      "Average Precision on Full Dataset: {0: 0.9566446358767136, 1: 0.31568739915531835}\n",
      "Average Recall on Full Dataset: {0: 0.7113367617674997, 1: 0.8050755389577322}\n",
      "Average F1-Score on Full Dataset: {0: 0.8159468723687965, 1: 0.45352674746959254}\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "y[y == 2] = 1\n",
    "y_full[y_full == 2] = 1\n",
    "\n",
    "clf = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, objective='multi:softmax', num_class=2)\n",
    "avg_precision = {0: 0, 1: 0}\n",
    "avg_recall = {0: 0, 1: 0}\n",
    "avg_f1_score = {0: 0, 1: 0}\n",
    "# Initialize accumulators for the full dataset\n",
    "avg_precision_full = {0: 0, 1: 0}\n",
    "avg_recall_full = {0: 0, 1: 0}\n",
    "avg_f1_score_full = {0: 0, 1: 0}\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average=None, zero_division=1)\n",
    "    class_to_index = {label: index for index, label in enumerate(sorted(np.unique(y_test)))}\n",
    "    # Accumulate metrics for averaging\n",
    "    for i in [0, 1]:\n",
    "        idx= class_to_index[i]\n",
    "        avg_precision[i] += precision[idx]\n",
    "        avg_recall[i] += recall[idx]\n",
    "        avg_f1_score[i] += f1_score[idx]\n",
    "    #FULL added\n",
    "    y_pred_full = clf.predict(X_full)\n",
    "    # Calculate metrics for the full dataset\n",
    "    precision_full, recall_full, f1_score_full, _ = precision_recall_fscore_support(y_full, y_pred_full, average=None)\n",
    "    class_to_index_full = {label: index for index, label in enumerate(sorted(np.unique(y_full)))}\n",
    "    \n",
    "    # Accumulate metrics for averaging (full dataset)\n",
    "    for i in [0, 1]:\n",
    "        idx_full = class_to_index_full[i]\n",
    "        avg_precision_full[i] += precision_full[idx_full]\n",
    "        avg_recall_full[i] += recall_full[idx_full]\n",
    "        avg_f1_score_full[i] += f1_score_full[idx_full]\n",
    "    \n",
    "    \n",
    "# Calculate average metrics\n",
    "for i in [0, 1]:\n",
    "    avg_precision[i] /= n_splits\n",
    "    avg_recall[i] /= n_splits\n",
    "    avg_f1_score[i] /= n_splits\n",
    "# Calculate average metrics for the full dataset\n",
    "for i in [0, 1]:\n",
    "    avg_precision_full[i] /= n_splits\n",
    "    avg_recall_full[i] /= n_splits\n",
    "    avg_f1_score_full[i] /= n_splits\n",
    "# Print average metrics\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1-Score:\", avg_f1_score)\n",
    "\n",
    "# Print average metrics for the full dataset\n",
    "print(\"Average Precision on Full Dataset:\", avg_precision_full)\n",
    "print(\"Average Recall on Full Dataset:\", avg_recall_full)\n",
    "print(\"Average F1-Score on Full Dataset:\", avg_f1_score_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5750b9a-8528-481f-91a5-7bcfe6c452dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T00:20:30.166397300Z",
     "start_time": "2023-10-27T00:20:30.144397900Z"
    }
   },
   "outputs": [],
   "source": [
    "# # TensorFlow Neural Network\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "# import numpy as np\n",
    "# \n",
    "# # Initialize metrics dictionary for averaging\n",
    "# avg_precision = {0: 0, 2: 0}\n",
    "# avg_recall = {0: 0, 2: 0}\n",
    "# avg_f1_score = {0: 0, 2: 0}\n",
    "# \n",
    "# # Initialize and compile the model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(12, input_dim=X.shape[1], activation='relu'))\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(3, activation='softmax'))\n",
    "# \n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# \n",
    "# # Loop for k-fold cross-validation\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     \n",
    "#     # Fit the model\n",
    "#     model.fit(X_train, tf.keras.utils.to_categorical(y_train, num_classes=3), epochs=50, batch_size=10, verbose=0)\n",
    "#     \n",
    "#     # Predict\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_pred_mapped = np.argmax(y_pred, axis=1)\n",
    "#     \n",
    "#     y_test_mapped = y_test  # Assuming y_test is already a 1-D array\n",
    "#     \n",
    "#     # Calculate metrics\n",
    "#     unique_classes = np.unique(y_test_mapped)\n",
    "#     precision, recall, f1_score, _ = precision_recall_fscore_support(y_test_mapped, y_pred_mapped, average=None, labels=unique_classes)\n",
    "#     \n",
    "#     # Create a mapping from class label to index\n",
    "#     class_to_index = {label: index for index, label in enumerate(unique_classes)}\n",
    "#     \n",
    "#     # Accumulate metrics for averaging\n",
    "#     for class_label in [0, 2]:\n",
    "#         idx = class_to_index.get(class_label, None)\n",
    "#         if idx is not None:\n",
    "#             avg_precision[class_label] += precision[idx]\n",
    "#             avg_recall[class_label] += recall[idx]\n",
    "#             avg_f1_score[class_label] += f1_score[idx]\n",
    "#         else:\n",
    "#             avg_precision[class_label] += 0\n",
    "#             avg_recall[class_label] += 0\n",
    "#             avg_f1_score[class_label] += 0\n",
    "#     \n",
    "# # Calculate average metrics\n",
    "# n_splits = 5  # Number of splits in k-fold\n",
    "# for class_label in [0, 2]:\n",
    "#     avg_precision[class_label] /= n_splits\n",
    "#     avg_recall[class_label] /= n_splits\n",
    "#     avg_f1_score[class_label] /= n_splits\n",
    "# \n",
    "# # Print average metrics\n",
    "# print(\"Average Precision:\", avg_precision)\n",
    "# print(\"Average Recall:\", avg_recall)\n",
    "# print(\"Average F1-Score:\", avg_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ac482b-f0dc-4f80-8cbd-ff1993846070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T00:20:30.173897Z",
     "start_time": "2023-10-27T00:20:30.158398200Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
